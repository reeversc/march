{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNRx/GYmv94vzhfzT4iXP3w"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install youtube_transcript_api -q"
      ],
      "metadata": {
        "id": "hMyd7mn14_DF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bddf5f77-bc95-41d3-a74f-237fd22d62eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/622.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.1/622.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m614.4/622.3 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m622.3/622.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPsEyQMp3JS4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe1e09b2-d104-497c-f5ee-1d3da068f4df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcript saved to transcript_graph.txt\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "from urllib.parse import urlparse, parse_qs\n",
        "\n",
        "def extract_video_id(url):\n",
        "    # Parsing the URL\n",
        "    parsed_url = urlparse(url)\n",
        "\n",
        "    # Extracting video ID from different URL formats\n",
        "    if parsed_url.hostname == 'youtu.be':\n",
        "        return parsed_url.path[1:]\n",
        "    if parsed_url.hostname in ('www.youtube.com', 'youtube.com'):\n",
        "        if parsed_url.path == '/watch':\n",
        "            return parse_qs(parsed_url.query)['v'][0]\n",
        "        if parsed_url.path[:7] == '/embed/':\n",
        "            return parsed_url.path.split('/')[2]\n",
        "        if parsed_url.path[:3] == '/v/':\n",
        "            return parsed_url.path.split('/')[2]\n",
        "    # If the video ID couldn't be extracted\n",
        "    return None\n",
        "\n",
        "def get_transcript(video_url):\n",
        "    video_id = extract_video_id(video_url)\n",
        "    if not video_id:\n",
        "        print(\"Could not extract video ID from the given URL.\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
        "        return transcript\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def save_transcript(transcript, filename):\n",
        "    if transcript:\n",
        "        with open(filename, 'w', encoding='utf-8') as file:\n",
        "            for entry in transcript:\n",
        "                file.write(f\"{entry['text']}\\n\")\n",
        "        print(f\"Transcript saved to {filename}\")\n",
        "    else:\n",
        "        print(\"No transcript to save.\")\n",
        "\n",
        "# Example usage\n",
        "video_url = \"https://youtu.be/8mfI0TRwR1E?si=J8fLG0wwFahA5FGb\"  # Replace with your desired YouTube video URL\n",
        "transcript = get_transcript(video_url)\n",
        "save_transcript(transcript, \"transcript_graph.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "from urllib.parse import urlparse, parse_qs\n",
        "\n",
        "def extract_video_id(url):\n",
        "    # Parsing the URL\n",
        "    parsed_url = urlparse(url)\n",
        "\n",
        "    # Extracting video ID from different URL formats\n",
        "    if parsed_url.hostname == 'youtu.be':\n",
        "        return parsed_url.path[1:]\n",
        "    if parsed_url.hostname in ('www.youtube.com', 'youtube.com'):\n",
        "        if parsed_url.path == '/watch':\n",
        "            return parse_qs(parsed_url.query)['v'][0]\n",
        "        if parsed_url.path[:7] == '/embed/':\n",
        "            return parsed_url.path.split('/')[2]\n",
        "        if parsed_url.path[:3] == '/v/':\n",
        "            return parsed_url.path.split('/')[2]\n",
        "    # If the video ID couldn't be extracted\n",
        "    return None\n",
        "\n",
        "def get_transcript(video_url):\n",
        "    video_id = extract_video_id(video_url)\n",
        "    if not video_id:\n",
        "        print(\"Could not extract video ID from the given URL.\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
        "        return transcript\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def save_transcript(transcript, filename):\n",
        "    if transcript:\n",
        "        with open(filename, 'w', encoding='utf-8') as file:\n",
        "            for entry in transcript:\n",
        "                file.write(f\"{entry['text']}\\n\")\n",
        "        print(f\"Transcript saved to {filename}\")\n",
        "    else:\n",
        "        print(\"No transcript to save.\")\n",
        "\n",
        "# Example usage\n",
        "video_url = \"https://www.youtube.com/watch?v=8mfI0TRwR1E\"  # Replace with your desired YouTube video URL\n",
        "transcript = get_transcript(video_url)\n",
        "save_transcript(transcript, \"transcript_4.txt\")"
      ],
      "metadata": {
        "id": "X7ckIIhQqMry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "\n",
        "def scrape_amazon_product(url):\n",
        "    # Headers to mimic a browser visit\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "        'Accept-Language': 'en-US,en;q=0.9',\n",
        "        'Accept-Encoding': 'gzip, deflate, br',\n",
        "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
        "        'Connection': 'keep-alive',\n",
        "        'Referer': 'https://www.google.com/'\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Add a random delay to avoid being detected as a bot\n",
        "        time.sleep(random.uniform(1, 3))\n",
        "\n",
        "        # Send the request to Amazon\n",
        "        response = requests.get(url, headers=headers)\n",
        "\n",
        "        if response.status_code != 200:\n",
        "            return {\"error\": f\"Failed to retrieve the page. Status code: {response.status_code}\"}\n",
        "\n",
        "        # Parse the HTML content\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Extract product information\n",
        "        product_data = {}\n",
        "\n",
        "        # Product title\n",
        "        product_title = soup.select_one('#productTitle')\n",
        "        if product_title:\n",
        "            product_data['title'] = product_title.get_text().strip()\n",
        "\n",
        "        # Price\n",
        "        price = soup.select_one('.a-price .a-offscreen')\n",
        "        if price:\n",
        "            product_data['price'] = price.get_text().strip()\n",
        "\n",
        "        # Rating\n",
        "        rating = soup.select_one('#acrPopover')\n",
        "        if rating:\n",
        "            product_data['rating'] = rating.get('title', '').strip()\n",
        "\n",
        "        # Number of reviews\n",
        "        review_count = soup.select_one('#acrCustomerReviewText')\n",
        "        if review_count:\n",
        "            product_data['review_count'] = review_count.get_text().strip()\n",
        "\n",
        "        # Availability\n",
        "        availability = soup.select_one('#availability')\n",
        "        if availability:\n",
        "            product_data['availability'] = availability.get_text().strip()\n",
        "\n",
        "        # Product description\n",
        "        description = soup.select_one('#productDescription')\n",
        "        if description:\n",
        "            product_data['description'] = description.get_text().strip()\n",
        "\n",
        "        # Product features\n",
        "        features = soup.select('#feature-bullets li')\n",
        "        if features:\n",
        "            product_data['features'] = [feature.get_text().strip() for feature in features if not feature.get('class')]\n",
        "\n",
        "        return product_data\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\"error\": str(e)}\n",
        "\n",
        "# Replace with your Amazon product URL\n",
        "amazon_url = \"https://www.amazon.com/dp/B0BXTCGP6B?ref=ppx_yo2ov_dt_b_fed_asin_title\"\n",
        "\n",
        "result = scrape_amazon_product(amazon_url)\n",
        "print(json.dumps(result, indent=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkIW96i71wIM",
        "outputId": "fdc8b9cb-c927-4dcd-f82f-a0e080031b29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"title\": \"Earth Harmony Methylene Blue 1% Pharmaceutical Grade Supplement with Dropper, Includes Nano Liquid Gold, Methylene Blue USP Grade, No Formaldehyde (2 Oz)\",\n",
            "  \"price\": \"$30.90\",\n",
            "  \"rating\": \"4.4 out of 5 stars\",\n",
            "  \"review_count\": \"1,103 ratings\",\n",
            "  \"availability\": \"In Stock\",\n",
            "  \"features\": []\n",
            "}\n"
          ]
        }
      ]
    }
  ]
}